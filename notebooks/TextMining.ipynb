{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features de TextMining:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import datetime\n",
    "import dateutil\n",
    "\n",
    "sys.path.insert(0,os.path.dirname(os.getcwd()))\n",
    "sys.path.insert(0,os.path.join(os.getcwd(),'grobid'))\n",
    "sys.path.insert(0,os.getcwd())\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from grobid import grobid_client\n",
    "import grobid_tei_xml\n",
    "from grobid_to_dataframe import grobid_cli, xmltei_to_dataframe\n",
    "\n",
    "import plotly\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from pyvis.network import Network\n",
    "import nltk\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install markupsafe==2.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comandos Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "docker run -t --rm --init -p 8080:8070 -p 8081:8071 --memory=\"9g\" lfoppiano/grobid:0.7.0\n",
    "\n",
    "docker run -t --rm --init -p 8080:8070 -p 8081:8071 lfoppiano/grobid:0.6.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definindo variáveis e caminhos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.dirname(os.getcwd())\n",
    "path_input = os.path.join(path,'artifacts','articles','ml_material','teste')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções para execução em batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(path_input_path):\n",
    "    \"\"\"\"\"\"\n",
    "    if os.path.exists(path_input_path):\n",
    "        return path_input_path\n",
    "    \n",
    "    return os.getcwd()\n",
    "\n",
    "\n",
    "def batch_process_path(path_input_path, n_workers=2,\n",
    "                       check_cache=True,\n",
    "                       cache_folder_name='summarticles_cache',\n",
    "                       config_path=\"./grobid/config.json\"):\n",
    "    \n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    gcli = grobid_cli(config_path=config_path)\n",
    "    result_batch = gcli.process_pdfs(input_path=path_input_path,\n",
    "                                     check_cache=check_cache,\n",
    "                                     cache_folder_name=cache_folder_name,\n",
    "                                     n_workers=n_workers,\n",
    "                                     service=\"processFulltextDocument\",\n",
    "                                     generateIDs=True,\n",
    "                                     include_raw_citations=True,\n",
    "                                     include_raw_affiliations=True,\n",
    "                                     consolidate_header=False,\n",
    "                                     consolidate_citations=False,\n",
    "                                     tei_coordinates=False,\n",
    "                                     segment_sentences=True,\n",
    "                                     verbose=True)\n",
    "    return result_batch\n",
    "\n",
    "\n",
    "def get_dataframes(result_batch):\n",
    "    \n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    xml_to_df = xmltei_to_dataframe()\n",
    "    dict_dfs, dic_errors = xml_to_df.get_dataframe_articles(result_batch)\n",
    "    \n",
    "    return dict_dfs, dic_errors\n",
    "\n",
    "\n",
    "def files_path(path):\n",
    "    list_dir = os.listdir(path)\n",
    "    files = []\n",
    "    for file in list_dir:\n",
    "        if os.path.isfile(os.path.join(path,file)):\n",
    "            files.append(os.path.join(path,file))\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch_process(path_input, n_workers=6, check_cache=True, \n",
    "                      cache_folder_name='summarticles_cache', \n",
    "                      config_path=\"./grobid/config.json\"):\n",
    "\n",
    "    dict_exec = {'path':path_input}\n",
    "    dict_exec['start_datetime'] = datetime.datetime.now()\n",
    "    \n",
    "    # path_input = os.path.join(path,'artifacts','test_article')\n",
    "    config_path = os.path.join(os.getcwd(),'grobid','config.json')\n",
    "    dict_exec['grobid_config'] = config_path\n",
    "    \n",
    "    gcli = grobid_client.GrobidClient(config_path=config_path, check_server=False)\n",
    "    \n",
    "    dict_exec['files'] = gcli.get_input_files(path_input)\n",
    "    dict_exec['num_files'] = len(dict_exec['files'])\n",
    "    dict_exec['n_workers'] = n_workers\n",
    "    \n",
    "    path_input_path = get_path(path_input)\n",
    "    result_batch = batch_process_path(path_input_path, n_workers=dict_exec['n_workers'], check_cache=check_cache)\n",
    "    dict_dfs, dic_errors = get_dataframes(result_batch)\n",
    "    \n",
    "    gcli.save_xmltei_files(result_batch, input_folder_path, cache_folder_name=cache_folder_name)\n",
    "    \n",
    "    dict_exec['end_datetime'] = datetime.datetime.now()\n",
    "    dict_exec['time_exec_sec'] = (dict_exec['end_datetime']-dict_exec['start_datetime']).seconds\n",
    "    dict_exec['time_exec_min'] = (dict_exec['end_datetime']-dict_exec['start_datetime']).seconds\n",
    "    \n",
    "    return dict_dfs, dict_exec, dic_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder_path = r\"\"\"C:\\Users\\vierb\\OneDrive\\Área de Trabalho\\Projetos\\PGC\\artifacts\\articles\\ml_material\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROBID server is up and running\n",
      "587 files to process in current batch\n",
      "[Input Files] 587\n",
      "[Cache Files] 587\n",
      "In the end, we have: 0  new files to process!\n",
      "And we have : 587  files to back from cache!\n",
      "Processed articles: 581\n",
      "Number articles with errors: 6\n",
      "Wall time: 23.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dict_dfs, dict_exec, dic_errors = run_batch_process(path_input=input_folder_path, \n",
    "                                                    n_workers=10, \n",
    "                                                    check_cache=True, \n",
    "                                                    cache_folder_name='summarticles_cache', \n",
    "                                                    config_path=\"./grobid/config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trabalhando no tratamento do texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#import spacy\n",
    "#import corenlp\n",
    "#import textblob\n",
    "#import gensim\n",
    "#import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tratando texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_tokenize(text, language='english', preserve_line=False):\n",
    "    return nltk.tokenize.word_tokenize(text, language=language, preserve_line=preserve_line)\n",
    "\n",
    "def clean_text_regex(words_list, regex=\"[^a-zA-Z]+\", replace='', min_word_len=1):\n",
    "    \"\"\"Testado em https://regex101.com/\"\"\"\n",
    "    new_words = []\n",
    "    for word in words_list:\n",
    "        word = re.sub(regex, replace, word)\n",
    "        if len(word) > min_word_len:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words_list, stopwords_list):\n",
    "    \"\"\"\"\"\"\n",
    "    new_words = []\n",
    "    for word in words_list:\n",
    "        if word not in stopwords_list:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def lemmatizer(words_list):\n",
    "    \"\"\"\"\"\"\n",
    "    obj_lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    words_lemma = []\n",
    "    for word in words_list:\n",
    "        words_lemma.append(obj_lemmatizer.lemmatize(word,pos=nltk.corpus.wordnet.VERB))\n",
    "    return words_lemma\n",
    "\n",
    "def stem_text(words_list):\n",
    "    \"\"\"\"\"\"\n",
    "    p_stem = nltk.stem.PorterStemmer()\n",
    "    words_stem = []\n",
    "    for word in words_list:\n",
    "        words_stem.append(p_stem.stem(word))\n",
    "    return words_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_prep(text, clean_text=True, stopwords_remove=True, exec_lemmatizer=True, exec_stem=False, text_lower=False, stopwords_list=[], language='english',\n",
    "              preserve_line=False, regex_chars_clean=\"[^a-zA-Z]+\", replace_chars_clean='', min_word_len=1):\n",
    "    \n",
    "    \"\"\"Text preparation.\"\"\"\n",
    "    \n",
    "    text_preparation = text_tokenize(text, language=language, preserve_line=preserve_line)\n",
    "    if clean_text:\n",
    "        text_preparation = clean_text_regex(words_list=text_preparation,\n",
    "                                            regex=regex_chars_clean,\n",
    "                                            replace=replace_chars_clean,\n",
    "                                            min_word_len=min_word_len)\n",
    "    if stopwords_remove:\n",
    "        text_preparation = remove_stopwords(words_list=text_preparation,\n",
    "                                            stopwords_list=stopwords_list)\n",
    "    if exec_lemmatizer:\n",
    "        text_preparation = lemmatizer(words_list=text_preparation)\n",
    "    if exec_stem:\n",
    "        text_preparation = stem_text(words_list=text_preparation)\n",
    "    text_preparation = ' '.join(text_preparation)\n",
    "    if text_lower:\n",
    "        text_preparation = text_preparation.lower()\n",
    "    return text_preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_prep_column(colum_df):\n",
    "    \"\"\"\"\"\"\n",
    "    f_prep_text = lambda text_data: text_prep(text=text_data, clean_text=True, stopwords_remove=True, exec_lemmatizer=True, exec_stem=False, \n",
    "                                            stopwords_list=nltk.corpus.stopwords.words('english'), language='english', preserve_line=False,\n",
    "                                            regex_chars_clean=\"[^a-zA-Z]+\", replace_chars_clean='', min_word_len=1, text_lower=True)\n",
    "    colum_df = colum_df.apply(lambda e: e if pd.isna(e) else f_prep_text(e))\n",
    "    return colum_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_dfs['df_doc_info']['acknowledgement_prep'] = text_prep_column(dict_dfs['df_doc_info']['acknowledgement'])\n",
    "dict_dfs['df_doc_info']['abstract_prep'] = text_prep_column(dict_dfs['df_doc_info']['abstract'])\n",
    "dict_dfs['df_doc_info']['body_prep'] = text_prep_column(dict_dfs['df_doc_info']['body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando BOW e TFIDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = dict_dfs['df_doc_info']['abstract_prep'].fillna(' ').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_bow = CountVectorizer(encoding=\"utf-8\",\n",
    "                          stop_words=\"english\",\n",
    "                          strip_accents=\"ascii\",\n",
    "                          lowercase=True, \n",
    "                          preprocessor=None,\n",
    "                          tokenizer=None,\n",
    "                          token_pattern=r\"\"\"(?u)\\b\\w\\w+\\b\"\"\",\n",
    "                          ngram_range=(1,2), # Unigram and bigram\n",
    "                          analyzer=\"word\",\n",
    "                          max_df=1.0,\n",
    "                          min_df=2, # May have at least 2 frequency\n",
    "                          max_features=None, \n",
    "                          vocabulary= None, \n",
    "                          binary=False, \n",
    "                          dtype=np.int64)\n",
    "\n",
    "obj_tfidf = TfidfVectorizer(encoding=\"utf-8\",\n",
    "                            stop_words=\"english\",\n",
    "                            strip_accents=\"ascii\",\n",
    "                            lowercase=True, \n",
    "                            preprocessor=None,\n",
    "                            tokenizer=None,\n",
    "                            token_pattern=r\"\"\"(?u)\\b\\w\\w+\\b\"\"\",\n",
    "                            ngram_range=(1,2), # Unigram and bigram\n",
    "                            analyzer=\"word\",\n",
    "                            max_df=1.0,\n",
    "                            min_df=2, # May have at least 2 frequency\n",
    "                            max_features=None, \n",
    "                            vocabulary= None, \n",
    "                            binary=False, \n",
    "                            dtype=np.float64, \n",
    "                            norm='l2', \n",
    "                            use_idf=True, \n",
    "                            smooth_idf=True, \n",
    "                            sublinear_tf=False)\n",
    "\n",
    "obj_bow = obj_bow.fit(raw_documents=documents)\n",
    "bow_matrix = obj_bow.transform(documents)\n",
    "\n",
    "obj_tfidf = obj_tfidf.fit(raw_documents=documents)\n",
    "tfidf_matrix = obj_tfidf.transform(documents)\n",
    "\n",
    "bow_matrix = bow_matrix.todense()\n",
    "tfidf_matrix = tfidf_matrix.todense()\n",
    "\n",
    "df_bow = pd.DataFrame(bow_matrix, columns=obj_bow.get_feature_names())\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix, columns=obj_tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trabalhando com a similaridade de Cosseno:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(581, 7503)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_bow_sim = cosine_similarity(df_bow, df_bow)\n",
    "cos_tfidf_sim = cosine_similarity(df_tfidf, df_tfidf)\n",
    "\n",
    "df_cos_bow_sim = pd.DataFrame(cos_bow_sim,\n",
    "                              columns=dict_dfs['df_doc_info'].index.tolist(),\n",
    "                              index=dict_dfs['df_doc_info'].index.tolist())\n",
    "\n",
    "df_cos_tfidf_sim = pd.DataFrame(cos_tfidf_sim,\n",
    "                                columns=dict_dfs['df_doc_info'].index.tolist(),\n",
    "                                index=dict_dfs['df_doc_info'].index.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gerando grafo de similaridade de cosseno:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sim_matrix(matrix, percentil=\"75%\", value_min=0, value_max=1):\n",
    "    \"\"\"\"\"\"\n",
    "    list_elements = []\n",
    "    for colum in df_cos_tfidf_sim.columns.tolist():\n",
    "        list_elements += df_cos_tfidf_sim[colum].tolist()\n",
    "    sim_describe = pd.Series(list_elements).describe(percentiles=np.arange(0, 1, 0.001))\n",
    "    del list_elements\n",
    "    \n",
    "    filter_matrix = sim_describe[percentil]\n",
    "    \n",
    "    list_filter = []\n",
    "    for i,row in matrix.iterrows():\n",
    "        for j in row.index:\n",
    "            value = matrix.loc[i,j]\n",
    "            logic_filter = value>=filter_matrix and value>value_min and value<value_max\n",
    "            if not pd.isna(value) and logic_filter:\n",
    "                dictCell = {\"doc_a\":i,\"doc_b\":j,'value':matrix.loc[i,j]}\n",
    "                list_filter.append(dictCell)\n",
    "    df_maxtrix_filter = pd.DataFrame(list_filter)\n",
    "    del list_filter\n",
    "    \n",
    "    return df_maxtrix_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sim_graph(matrix,\n",
    "                   node_data,\n",
    "                   source_column=\"doc_a\",\n",
    "                   to_column=\"doc_b\",\n",
    "                   value_column=\"value\",\n",
    "                   height=\"500px\",\n",
    "                   width=\"500px\",\n",
    "                   directed=False,\n",
    "                   notebook=False,\n",
    "                   bgcolor=\"#ffffff\",\n",
    "                   font_color=False,\n",
    "                   layout=None,\n",
    "                   heading=\"\",\n",
    "                   path_graph=\"./\", \n",
    "                   name_file=\"graph.html\"):\n",
    "    \"\"\"\"\"\"\n",
    "    graph = Network(height=height,\n",
    "                    width=width,\n",
    "                    directed=directed,\n",
    "                    notebook=notebook,\n",
    "                    bgcolor=bgcolor,\n",
    "                    font_color=font_color,\n",
    "                    layout=layout,\n",
    "                    heading=heading)\n",
    "\n",
    "    for i, row in node_data.iterrows():\n",
    "        \n",
    "        article_id = str(row['article_id'])\n",
    "        article_title = str(row['title_head'])\n",
    "        article_abstract_short = str(row['abstract_short'])\n",
    "        article_date = str(row['date_head'])\n",
    "        article_number_authors = str(row['author_count'])\n",
    "        article_number_citations = str(row['citation_count'])\n",
    "        article_doi = str(row['doi_head'])\n",
    "        article_file_name = str(row['file_name'])\n",
    "        article_file_path = str(row['file'])\n",
    "        \n",
    "        title_html = f\"\"\"Article Title:{article_title}\n",
    "                         Article Date:{article_date}\n",
    "                         Article Number Authors:{article_number_authors}\n",
    "                         Article Number Citations:{article_number_citations}\n",
    "                         Article DOI:{article_doi}\n",
    "                         Article File Name:{article_file_name}\"\"\"\n",
    "        \n",
    "        graph.add_node(n_id=article_id, \n",
    "                       label=f\"Node ID: {str(article_id)[0:4]}\", \n",
    "                       borderWidth=1, \n",
    "                       borderWidthSelected=2, \n",
    "                       #brokenImage=\"url\", \n",
    "                       #group=\"a\", \n",
    "                       #hidden=False, \n",
    "                       #image=\"url\", \n",
    "                       #labelHighlightBold=True, \n",
    "                       #level=1, \n",
    "                       #mass=1, \n",
    "                       #physics=True,\n",
    "                       shape=\"dot\", # image, circularImage, diamond, dot, star, triangle, triangleDown, square and icon\n",
    "                       size=1, \n",
    "                       title=title_html,  \n",
    "                       #x=0.5, \n",
    "                       #y=1.0)\n",
    "                       value=1)\n",
    "        \n",
    "    for i,row in matrix.iterrows():\n",
    "        \n",
    "        graph.add_edge(source=row[source_column],\n",
    "                       to=row[to_column],\n",
    "                       value=round(row[value_column],1),\n",
    "                       title=row[value_column])\n",
    "                       #width=row['value'],\n",
    "                       #arrowStrikethrough=False,\n",
    "                       #physics=False,\n",
    "                       #hidden=False)\n",
    "    \n",
    "    graph.force_atlas_2based(gravity=-50,\n",
    "                             central_gravity=0.01,\n",
    "                             spring_length=360,\n",
    "                             spring_strength=0.08,\n",
    "                             damping=0.4,\n",
    "                             overlap=0)\n",
    "    \n",
    "    graph.save_graph(os.path.join(path_graph, name_file))\n",
    "    graph.show_buttons(filter_=['physics'])\n",
    "    graph.show(name_file)\n",
    "    \n",
    "    return graph   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cos_bow_sim_filter = filter_sim_matrix(df_cos_bow_sim, percentil=\"99%\", value_min=0, value_max=0.99)\n",
    "df_cos_bow_sim_filter = df_cos_bow_sim_filter.nlargest(300,'value')\n",
    "\n",
    "df_cos_tfidf_sim_filter = filter_sim_matrix(df_cos_tfidf_sim, percentil=\"99%\", value_min=0, value_max=0.99)\n",
    "df_cos_tfidf_sim_filter = df_cos_tfidf_sim_filter.nlargest(300,'value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_data(dict_dfs):\n",
    "    \n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    # Selecting head article data\n",
    "    cols_head = ['title_head', 'doi_head', 'date_head',]\n",
    "    head_data = dict_dfs['df_doc_head'].loc[:,cols_head].reset_index().copy()\n",
    "    head_data['title_head'] = head_data['title_head'].apply(lambda e: str(e)[0:50] + \"...\" if len(str(e)) > 50 else str(e))\n",
    "\n",
    "    # Selecting head article data\n",
    "    cols_info = ['abstract','file']\n",
    "    doc_info_data = dict_dfs['df_doc_info'].loc[:,cols_info].reset_index().copy()\n",
    "    doc_info_data['file_name'] = doc_info_data['file'].apply(lambda e: os.path.split(e)[-1])\n",
    "    doc_info_data['abstract_short'] = doc_info_data['abstract'].apply(lambda e: str(e)[0:20] + \"...\" if len(str(e)) > 20 else str(e))\n",
    "    doc_info_data.drop(labels=['abstract'], axis=1, inplace=True)\n",
    "\n",
    "    # Selecting authors information\n",
    "    authors_data = dict_dfs['df_doc_authors'].reset_index()\n",
    "    authors_data = authors_data.groupby(by=['article_id'], as_index=False)['full_name_author'].count()\n",
    "    authors_data.rename(columns={'full_name_author':'author_count'}, inplace=True)\n",
    "\n",
    "    # Selecting citations information\n",
    "    citations_data = dict_dfs['df_doc_citations'].reset_index()\n",
    "    citations_data = citations_data.groupby(by=['article_id'], as_index=False)['index_citation'].count()\n",
    "    citations_data.rename(columns={'index_citation':'citation_count'}, inplace=True)\n",
    "\n",
    "    nodes = dict_dfs['df_doc_info'].reset_index()['article_id'].tolist()\n",
    "    df_nodes = pd.DataFrame(nodes, columns=['article_id'])\n",
    "\n",
    "    df_nodes = df_nodes.merge(head_data, how='left', on='article_id')\n",
    "    df_nodes = df_nodes.merge(doc_info_data, how='left', on='article_id')\n",
    "    df_nodes = df_nodes.merge(authors_data, how='left', on='article_id')\n",
    "    df_nodes = df_nodes.merge(citations_data, how='left', on='article_id')\n",
    "    \n",
    "    return df_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting head article data\n",
    "cols_head = ['title_head', 'doi_head', 'date_head',]\n",
    "head_data = dict_dfs['df_doc_head'].loc[:,cols_head].reset_index().copy()\n",
    "head_data['title_head'] = head_data['title_head'].apply(lambda e: str(e)[0:50] + \"...\" if len(str(e)) > 50 else str(e))\n",
    "\n",
    "# Selecting head article data\n",
    "cols_info = ['abstract','file']\n",
    "doc_info_data = dict_dfs['df_doc_info'].loc[:,cols_info].reset_index().copy()\n",
    "doc_info_data['file_name'] = doc_info_data['file'].apply(lambda e: os.path.split(e)[-1])\n",
    "doc_info_data['abstract_short'] = doc_info_data['abstract'].apply(lambda e: str(e)[0:20] + \"...\" if len(str(e)) > 20 else str(e))\n",
    "doc_info_data.drop(labels=['abstract'], axis=1, inplace=True)\n",
    "\n",
    "# Selecting authors information\n",
    "authors_data = dict_dfs['df_doc_authors'].reset_index()\n",
    "authors_data = authors_data.groupby(by=['article_id'], as_index=False)['full_name_author'].count()\n",
    "authors_data.rename(columns={'full_name_author':'author_count'}, inplace=True)\n",
    "\n",
    "# Selecting citations information\n",
    "citations_data = dict_dfs['df_doc_citations'].reset_index()\n",
    "citations_data = citations_data.groupby(by=['article_id'], as_index=False)['index_citation'].count()\n",
    "citations_data.rename(columns={'index_citation':'citation_count'}, inplace=True)\n",
    "\n",
    "nodes = list(set(df_cos_bow_sim_filter.doc_a.tolist()+df_cos_bow_sim_filter.doc_b.tolist()))\n",
    "df_nodes = pd.DataFrame(nodes, columns=['article_id'])\n",
    "\n",
    "df_nodes = df_nodes.merge(head_data, how='left', on='article_id')\n",
    "df_nodes = df_nodes.merge(doc_info_data, how='left', on='article_id')\n",
    "df_nodes = df_nodes.merge(authors_data, how='left', on='article_id')\n",
    "df_nodes = df_nodes.merge(citations_data, how='left', on='article_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_graph = make_sim_graph(matrix=df_cos_bow_sim_filter,\n",
    "                           node_data=df_nodes,\n",
    "                           source_column=\"doc_a\",\n",
    "                           to_column=\"doc_b\",\n",
    "                           value_column=\"value\",\n",
    "                           height=\"1000px\",\n",
    "                           width=\"1000px\",\n",
    "                           directed=True,\n",
    "                           notebook=False,\n",
    "                           bgcolor=\"#ffffff\",\n",
    "                           font_color=False,\n",
    "                           layout=None,\n",
    "                           heading=\"\",\n",
    "                           path_graph=\"./\", \n",
    "                           name_file=\"graph.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_model = KeyBERT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aleatory_color():\n",
    "    \n",
    "\t'''Returns color in hex format'''\n",
    " \n",
    "\tred_int = random.randint(0,255)\n",
    "\tgreen_int = random.randint(0,255)\n",
    "\tblue_int = random.randint(0,255)\n",
    " \n",
    "\treturn '#{:02X}{:02X}{:02X}'.format(red_int, green_int, blue_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_keywords_graph(edges_key_articles,\n",
    "                        node_data,\n",
    "                        node_keywords_data,\n",
    "                        source_column=\"keyword\",\n",
    "                        to_column=\"article_id\",\n",
    "                        value_column=\"value\",\n",
    "                        height=\"500px\",\n",
    "                        width=\"500px\",\n",
    "                        directed=False,\n",
    "                        notebook=False,\n",
    "                        bgcolor=\"#ffffff\",\n",
    "                        font_color=False,\n",
    "                        layout=None,\n",
    "                        heading=\"\",\n",
    "                        path_graph=\"./\", \n",
    "                        name_file=\"graph_keyword.html\"):\n",
    "    \n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    graph = Network(height=height,\n",
    "                    width=width,\n",
    "                    directed=directed,\n",
    "                    notebook=notebook,\n",
    "                    bgcolor=bgcolor,\n",
    "                    font_color=font_color,\n",
    "                    layout=layout,\n",
    "                    heading=heading)\n",
    "\n",
    "    for i, row in node_data.iterrows():\n",
    "        \n",
    "        article_id = str(row['article_id'])\n",
    "        article_title = str(row['title_head'])\n",
    "        article_abstract_short = str(row['abstract_short'])\n",
    "        article_date = str(row['date_head'])\n",
    "        article_number_authors = str(row['author_count'])\n",
    "        article_number_citations = str(row['citation_count'])\n",
    "        article_doi = str(row['doi_head'])\n",
    "        article_file_name = str(row['file_name'])\n",
    "        article_file_path = str(row['file'])\n",
    "        \n",
    "        title_html = f\"\"\"Article Title:{article_title}\n",
    "                         Article Date:{article_date}\n",
    "                         Article Number Authors:{article_number_authors}\n",
    "                         Article Number Citations:{article_number_citations}\n",
    "                         Article DOI:{article_doi}\n",
    "                         Article File Name:{article_file_name}\"\"\"\n",
    "        \n",
    "        graph.add_node(n_id=article_id, \n",
    "                       label=f\"Node ID: {str(article_id)[0:4]}\", \n",
    "                       borderWidth=1, \n",
    "                       borderWidthSelected=2, \n",
    "                       #brokenImage=\"url\", \n",
    "                       #group=\"a\", \n",
    "                       #hidden=False, \n",
    "                       #image=\"url\", \n",
    "                       #labelHighlightBold=True, \n",
    "                       #level=1, \n",
    "                       #mass=1, \n",
    "                       #physics=True,\n",
    "                       shape=\"dot\", # image, circularImage, diamond, dot, star, triangle, triangleDown, square and icon\n",
    "                       size=1, \n",
    "                       title=title_html,  \n",
    "                       #x=0.5, \n",
    "                       #y=1.0)\n",
    "                       value=1)\n",
    "        \n",
    "    for i, row in node_keywords_data.iterrows():\n",
    "        \n",
    "        keyword_id = str(row['keyword'])\n",
    "        article_count = row['article_count']\n",
    "        value_sum = row['value_sum']\n",
    "        value_mean = row['value_mean']\n",
    "        \n",
    "        title_html = f\"\"\"KeyWord: {keyword_id}\n",
    "                         Article Count: {article_count}\n",
    "                         Value Sum: {value_sum}\n",
    "                         Value Mean: {value_mean}\n",
    "                      \"\"\"\n",
    "        \n",
    "        graph.add_node(n_id=keyword_id, \n",
    "                       label=keyword_id, \n",
    "                       borderWidth=2, \n",
    "                       borderWidthSelected=4,\n",
    "                       color=get_aleatory_color(),\n",
    "                       #brokenImage=\"url\", \n",
    "                       #group=\"a\", \n",
    "                       #hidden=False, \n",
    "                       #image=\"url\", \n",
    "                       #labelHighlightBold=True, \n",
    "                       #level=1, \n",
    "                       #mass=1, \n",
    "                       #physics=True,\n",
    "                       shape=\"box\", # image, circularImage, diamond, dot, star, triangle, triangleDown, square and icon, box, text\n",
    "                       size=article_count, \n",
    "                       title=title_html,  \n",
    "                       #x=0.5, \n",
    "                       #y=1.0)\n",
    "                       value=article_count)\n",
    "    \n",
    "    for i, row in edges_key_articles.iterrows():\n",
    "        \n",
    "        graph.add_edge(source=row[source_column],\n",
    "                       to=row[to_column],\n",
    "                       value=round(row[value_column],1),\n",
    "                       title=row[value_column])\n",
    "                       #width=row['value'],\n",
    "                       #arrowStrikethrough=False,\n",
    "                       #physics=False,\n",
    "                       #hidden=False)\n",
    "    \n",
    "    graph.force_atlas_2based(gravity=-50,\n",
    "                             central_gravity=0.01,\n",
    "                             spring_length=360,\n",
    "                             spring_strength=0.08,\n",
    "                             damping=0.4,\n",
    "                             overlap=0)\n",
    "    \n",
    "    graph.save_graph(os.path.join(path_graph, name_file))\n",
    "    graph.show_buttons(filter_=['physics'])\n",
    "    graph.show(name_file)\n",
    "    \n",
    "    return graph   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_keywords = {}\n",
    "id_column = 'article_id'\n",
    "text_column = 'abstract'\n",
    "col_select = [id_column,text_column]\n",
    "docs = dict_dfs['df_doc_info'].reset_index().loc[:, col_select]\n",
    "\n",
    "list_keywordsdf = []\n",
    "list_keywordsdf_article = []\n",
    "for i, row in docs.iterrows():\n",
    "    \n",
    "    doc = str(row[text_column])\n",
    "    id = row[id_column]\n",
    "    \n",
    "    keywords_unigram = kw_model.extract_keywords(doc, keyphrase_ngram_range=(1, 1), stop_words='english', highlight=False, top_n=10)\n",
    "    if len(keywords_unigram):\n",
    "        df_unigram = pd.DataFrame([{'keyword':v[0],'value':v[1]} for v in keywords_unigram])\n",
    "    else:\n",
    "        df_unigram = pd.DataFrame([], columns=['keyword','value'])\n",
    "\n",
    "    keywords_bigram = kw_model.extract_keywords(doc, keyphrase_ngram_range=(2, 2), stop_words='english', highlight=False, top_n=10)\n",
    "    if len(keywords_bigram):\n",
    "        df_bigram = pd.DataFrame([{'keyword':v[0],'value':v[1]} for v in keywords_bigram])\n",
    "    else:\n",
    "        df_bigram = pd.DataFrame([], columns=['keyword','value'])\n",
    "\n",
    "    keywords_trigam = kw_model.extract_keywords(doc, keyphrase_ngram_range=(3, 3), stop_words='english', highlight=False, top_n=10)\n",
    "    if len(keywords_bigram):\n",
    "        df_trigram = pd.DataFrame([{'keyword':v[0],'value':v[1]} for v in keywords_trigam])\n",
    "    else:\n",
    "        df_trigram = pd.DataFrame([], columns=['keyword','value'])\n",
    "    \n",
    "    dict_keywords[id] = {'unigram':df_unigram, 'bigram':df_bigram, 'trigram':df_trigram}\n",
    "    \n",
    "    df_article_keywords = pd.concat([df_unigram, df_bigram, df_trigram])\n",
    "    df_article_keywords[id_column] = id\n",
    "    df_article_keywords = df_article_keywords.loc[:,[id_column,'keyword', 'value']].copy()\n",
    "    list_keywordsdf_article.append(df_article_keywords)\n",
    "    \n",
    "    df_unigram.rename(columns={'keyword':'keyword_unigram','value':'value_unigram'}, inplace=True)\n",
    "    df_bigram.rename(columns={'keyword':'keyword_bigram','value':'value_bigram'}, inplace=True)\n",
    "    df_trigram.rename(columns={'keyword':'keyword_trigram','value':'value_trigram'}, inplace=True)\n",
    "    \n",
    "    df_keywords_article = pd.concat([df_unigram, df_bigram, df_trigram], axis=1)\n",
    "    dict_keywords[id]['df_keywords'] = df_keywords_article\n",
    "    \n",
    "    list_keywordsdf.append(df_keywords_article)\n",
    "    \n",
    "df_keywords_all = pd.concat(list_keywordsdf)\n",
    "df_keywords_all.dropna(inplace=True)\n",
    "\n",
    "df_article_keywords_all = pd.concat(list_keywordsdf_article)\n",
    "df_article_keywords_all.dropna(inplace=True)\n",
    "\n",
    "df_keywords_unigram = df_keywords_all.groupby(by=['keyword_unigram'], as_index=False)['value_unigram'].sum()\n",
    "df_keywords_unigram.sort_values(by='value_unigram', ascending=False, inplace=True)\n",
    "\n",
    "df_keywords_bigram = df_keywords_all.groupby(by=['keyword_bigram'], as_index=False)['value_bigram'].sum()\n",
    "df_keywords_bigram.sort_values(by='value_bigram', ascending=False, inplace=True)\n",
    "\n",
    "df_keywords_trigram = df_keywords_all.groupby(by=['keyword_trigram'], as_index=False)['value_trigram'].sum()\n",
    "df_keywords_trigram.sort_values(by='value_trigram', ascending=False, inplace=True)\n",
    "\n",
    "df_keywords_all = pd.concat([df_keywords_unigram, df_keywords_bigram, df_keywords_trigram], axis=1)\n",
    "df_keywords_all = df_keywords_all.head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_keys_node_data(grupo):\n",
    "    \"\"\"\"\"\"\n",
    "    dictAgg = {}\n",
    "    dictAgg['keyword'] = grupo['keyword'].iat[0]\n",
    "    dictAgg['article_count'] = grupo['article_id'].shape[0]\n",
    "    dictAgg['value_sum'] = grupo['value'].sum()\n",
    "    dictAgg['value_mean'] = grupo['value'].mean()\n",
    "    \n",
    "    return pd.Series(dictAgg)\n",
    "\n",
    "df_keyword_data = df_article_keywords_all.groupby(by=['keyword'], as_index=False).apply(agg_keys_node_data)\n",
    "top_keywords = 5 # int(df_keyword_data.shape[0]*0.1)\n",
    "df_keyword_data = df_keyword_data.sort_values(by=['article_count'], ascending=False).head(top_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting edges that contains top keywords\n",
    "filtro = (df_article_keywords_all.keyword.isin(df_keyword_data.keyword.tolist()))\n",
    "df_art_key_all = df_article_keywords_all.loc[(filtro)].copy()\n",
    "\n",
    "# Selecting nodes in the list of selected edges\n",
    "df_nodes = get_node_data(dict_dfs)\n",
    "\n",
    "filtro = (df_nodes['article_id'].isin(df_art_key_all['article_id'].tolist()))\n",
    "df_nodes = df_nodes.loc[(filtro)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_graph = make_keywords_graph(edges_key_articles=df_art_key_all,\n",
    "                                     node_data=df_nodes,\n",
    "                                     node_keywords_data=df_keyword_data,\n",
    "                                     source_column=\"keyword\",\n",
    "                                     to_column=\"article_id\",\n",
    "                                     value_column=\"value\",\n",
    "                                     height=\"1000px\",\n",
    "                                     width=\"1000px\",\n",
    "                                     directed=False,\n",
    "                                     notebook=False,\n",
    "                                     bgcolor=\"#ffffff\",\n",
    "                                     font_color=False,\n",
    "                                     layout=None,\n",
    "                                     heading=\"\",\n",
    "                                     path_graph=\"./\", \n",
    "                                     name_file=\"graph_keyword.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering, DBSCAN, KMeans, OPTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'3.0'.isnumeric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'3.0'.isalnum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a', 'b', 'c')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(['a','b','c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 if not None else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>578</th>\n",
       "      <th>579</th>\n",
       "      <th>580</th>\n",
       "      <th>581</th>\n",
       "      <th>582</th>\n",
       "      <th>583</th>\n",
       "      <th>584</th>\n",
       "      <th>585</th>\n",
       "      <th>586</th>\n",
       "      <th>587</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.023705</td>\n",
       "      <td>0.034409</td>\n",
       "      <td>0.015649</td>\n",
       "      <td>0.068962</td>\n",
       "      <td>0.097053</td>\n",
       "      <td>0.059413</td>\n",
       "      <td>0.127211</td>\n",
       "      <td>0.027346</td>\n",
       "      <td>0.009553</td>\n",
       "      <td>...</td>\n",
       "      <td>0.202743</td>\n",
       "      <td>0.103658</td>\n",
       "      <td>0.038098</td>\n",
       "      <td>0.040833</td>\n",
       "      <td>0.017276</td>\n",
       "      <td>0.099217</td>\n",
       "      <td>0.061297</td>\n",
       "      <td>0.013927</td>\n",
       "      <td>0.019023</td>\n",
       "      <td>0.018065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.023705</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.041778</td>\n",
       "      <td>0.061357</td>\n",
       "      <td>0.023355</td>\n",
       "      <td>0.029053</td>\n",
       "      <td>0.045903</td>\n",
       "      <td>0.020282</td>\n",
       "      <td>0.018764</td>\n",
       "      <td>0.026786</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032104</td>\n",
       "      <td>0.016359</td>\n",
       "      <td>0.057231</td>\n",
       "      <td>0.029093</td>\n",
       "      <td>0.059929</td>\n",
       "      <td>0.052306</td>\n",
       "      <td>0.018559</td>\n",
       "      <td>0.078809</td>\n",
       "      <td>0.013450</td>\n",
       "      <td>0.046446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.034409</td>\n",
       "      <td>0.041778</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.008849</td>\n",
       "      <td>0.037860</td>\n",
       "      <td>0.011418</td>\n",
       "      <td>0.054120</td>\n",
       "      <td>0.032207</td>\n",
       "      <td>0.014547</td>\n",
       "      <td>0.010750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009019</td>\n",
       "      <td>0.012165</td>\n",
       "      <td>0.093185</td>\n",
       "      <td>0.040861</td>\n",
       "      <td>0.081941</td>\n",
       "      <td>0.021206</td>\n",
       "      <td>0.014841</td>\n",
       "      <td>0.045751</td>\n",
       "      <td>0.002874</td>\n",
       "      <td>0.021497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.015649</td>\n",
       "      <td>0.061357</td>\n",
       "      <td>0.008849</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.034722</td>\n",
       "      <td>0.047289</td>\n",
       "      <td>0.027021</td>\n",
       "      <td>0.062384</td>\n",
       "      <td>0.030573</td>\n",
       "      <td>0.024522</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008994</td>\n",
       "      <td>0.045085</td>\n",
       "      <td>0.025515</td>\n",
       "      <td>0.050818</td>\n",
       "      <td>0.020659</td>\n",
       "      <td>0.019445</td>\n",
       "      <td>0.064512</td>\n",
       "      <td>0.015750</td>\n",
       "      <td>0.033434</td>\n",
       "      <td>0.083383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.068962</td>\n",
       "      <td>0.023355</td>\n",
       "      <td>0.037860</td>\n",
       "      <td>0.034722</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.055285</td>\n",
       "      <td>0.016076</td>\n",
       "      <td>0.049017</td>\n",
       "      <td>0.035053</td>\n",
       "      <td>0.015477</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032229</td>\n",
       "      <td>0.012874</td>\n",
       "      <td>0.049770</td>\n",
       "      <td>0.034738</td>\n",
       "      <td>0.027238</td>\n",
       "      <td>0.048417</td>\n",
       "      <td>0.059955</td>\n",
       "      <td>0.010112</td>\n",
       "      <td>0.014976</td>\n",
       "      <td>0.025576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 581 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          1         2         3         4         5         6         7  \\\n",
       "1  1.000000  0.023705  0.034409  0.015649  0.068962  0.097053  0.059413   \n",
       "2  0.023705  1.000000  0.041778  0.061357  0.023355  0.029053  0.045903   \n",
       "3  0.034409  0.041778  1.000000  0.008849  0.037860  0.011418  0.054120   \n",
       "4  0.015649  0.061357  0.008849  1.000000  0.034722  0.047289  0.027021   \n",
       "5  0.068962  0.023355  0.037860  0.034722  1.000000  0.055285  0.016076   \n",
       "\n",
       "          8         9        10  ...       578       579       580       581  \\\n",
       "1  0.127211  0.027346  0.009553  ...  0.202743  0.103658  0.038098  0.040833   \n",
       "2  0.020282  0.018764  0.026786  ...  0.032104  0.016359  0.057231  0.029093   \n",
       "3  0.032207  0.014547  0.010750  ...  0.009019  0.012165  0.093185  0.040861   \n",
       "4  0.062384  0.030573  0.024522  ...  0.008994  0.045085  0.025515  0.050818   \n",
       "5  0.049017  0.035053  0.015477  ...  0.032229  0.012874  0.049770  0.034738   \n",
       "\n",
       "        582       583       584       585       586       587  \n",
       "1  0.017276  0.099217  0.061297  0.013927  0.019023  0.018065  \n",
       "2  0.059929  0.052306  0.018559  0.078809  0.013450  0.046446  \n",
       "3  0.081941  0.021206  0.014841  0.045751  0.002874  0.021497  \n",
       "4  0.020659  0.019445  0.064512  0.015750  0.033434  0.083383  \n",
       "5  0.027238  0.048417  0.059955  0.010112  0.014976  0.025576  \n",
       "\n",
       "[5 rows x 581 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cos_tfidf_sim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "c1a9770e27579e7348d9a860a8519a7eb2fc807ceb8202e6d163f2591bed9881"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
