{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features de TextMining:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import datetime\n",
    "import dateutil\n",
    "\n",
    "sys.path.insert(0,os.path.dirname(os.getcwd()))\n",
    "sys.path.insert(0,os.path.join(os.getcwd(),'grobid'))\n",
    "sys.path.insert(0,os.getcwd())\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from grobid import grobid_client\n",
    "import grobid_tei_xml\n",
    "from grobid_to_dataframe import grobid_cli, xmltei_to_dataframe\n",
    "\n",
    "import plotly\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from pyvis.network import Network\n",
    "import nltk\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comandos Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "docker run -t --rm --init -p 8080:8070 -p 8081:8071 --memory=\"9g\" lfoppiano/grobid:0.7.0\n",
    "\n",
    "docker run -t --rm --init -p 8080:8070 -p 8081:8071 lfoppiano/grobid:0.6.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definindo variáveis e caminhos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.dirname(os.getcwd())\n",
    "path_input = os.path.join(path,'artifacts','articles','ml_material','teste')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções para execução em batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(path_input_path):\n",
    "    \"\"\"\"\"\"\n",
    "    if os.path.exists(path_input_path):\n",
    "        return path_input_path\n",
    "    \n",
    "    return os.getcwd()\n",
    "\n",
    "\n",
    "def batch_process_path(path_input_path, n_workers=2,\n",
    "                       check_cache=True,\n",
    "                       cache_folder_name='summarticles_cache',\n",
    "                       config_path=\"./grobid/config.json\"):\n",
    "    \n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    gcli = grobid_cli(config_path=config_path)\n",
    "    result_batch = gcli.process_pdfs(input_path=path_input_path,\n",
    "                                     check_cache=check_cache,\n",
    "                                     cache_folder_name=cache_folder_name,\n",
    "                                     n_workers=n_workers,\n",
    "                                     service=\"processFulltextDocument\",\n",
    "                                     generateIDs=True,\n",
    "                                     include_raw_citations=True,\n",
    "                                     include_raw_affiliations=True,\n",
    "                                     consolidate_header=False,\n",
    "                                     consolidate_citations=False,\n",
    "                                     tei_coordinates=False,\n",
    "                                     segment_sentences=True,\n",
    "                                     verbose=True)\n",
    "    return result_batch\n",
    "\n",
    "\n",
    "def get_dataframes(result_batch):\n",
    "    \n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    xml_to_df = xmltei_to_dataframe()\n",
    "    dict_dfs, dic_errors = xml_to_df.get_dataframe_articles(result_batch)\n",
    "    \n",
    "    return dict_dfs, dic_errors\n",
    "\n",
    "\n",
    "def files_path(path):\n",
    "    list_dir = os.listdir(path)\n",
    "    files = []\n",
    "    for file in list_dir:\n",
    "        if os.path.isfile(os.path.join(path,file)):\n",
    "            files.append(os.path.join(path,file))\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch_process(path_input, n_workers=6, check_cache=True, \n",
    "                      cache_folder_name='summarticles_cache', \n",
    "                      config_path=\"./grobid/config.json\"):\n",
    "\n",
    "    dict_exec = {'path':path_input}\n",
    "    dict_exec['start_datetime'] = datetime.datetime.now()\n",
    "    \n",
    "    # path_input = os.path.join(path,'artifacts','test_article')\n",
    "    config_path = os.path.join(os.getcwd(),'grobid','config.json')\n",
    "    dict_exec['grobid_config'] = config_path\n",
    "    \n",
    "    gcli = grobid_client.GrobidClient(config_path=config_path, check_server=False)\n",
    "    \n",
    "    dict_exec['files'] = gcli.get_input_files(path_input)\n",
    "    dict_exec['num_files'] = len(dict_exec['files'])\n",
    "    dict_exec['n_workers'] = n_workers\n",
    "    \n",
    "    path_input_path = get_path(path_input)\n",
    "    result_batch = batch_process_path(path_input_path, n_workers=dict_exec['n_workers'], check_cache=check_cache)\n",
    "    dict_dfs, dic_errors = get_dataframes(result_batch)\n",
    "    \n",
    "    gcli.save_xmltei_files(result_batch, input_folder_path, cache_folder_name=cache_folder_name)\n",
    "    \n",
    "    dict_exec['end_datetime'] = datetime.datetime.now()\n",
    "    dict_exec['time_exec_sec'] = (dict_exec['end_datetime']-dict_exec['start_datetime']).seconds\n",
    "    dict_exec['time_exec_min'] = (dict_exec['end_datetime']-dict_exec['start_datetime']).seconds\n",
    "    \n",
    "    return dict_dfs, dict_exec, dic_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder_path = r\"\"\"C:\\Users\\vierb\\OneDrive\\Área de Trabalho\\Projetos\\PGC\\artifacts\\articles\\ml_material\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROBID server is up and running\n",
      "587 files to process in current batch\n",
      "[Input Files] 587\n",
      "[Cache Files] 587\n",
      "In the end, we have: 0  new files to process!\n",
      "And we have : 587  files to back from cache!\n",
      "Processed articles: 581\n",
      "Number articles with errors: 6\n",
      "Wall time: 23.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dict_dfs, dict_exec, dic_errors = run_batch_process(path_input=input_folder_path, \n",
    "                                                    n_workers=10, \n",
    "                                                    check_cache=True, \n",
    "                                                    cache_folder_name='summarticles_cache', \n",
    "                                                    config_path=\"./grobid/config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trabalhando no tratamento do texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import corenlp\n",
    "import textblob\n",
    "import gensim\n",
    "#import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tratando texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_tokenize(text, language='english', preserve_line=False):\n",
    "    return nltk.tokenize.word_tokenize(text, language=language, preserve_line=preserve_line)\n",
    "\n",
    "def clean_text_regex(words_list, regex=\"[^a-zA-Z]+\", replace='', min_word_len=1):\n",
    "    \"\"\"Testado em https://regex101.com/\"\"\"\n",
    "    new_words = []\n",
    "    for word in words_list:\n",
    "        word = re.sub(regex, replace, word)\n",
    "        if len(word) > min_word_len:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words_list, stopwords_list):\n",
    "    \"\"\"\"\"\"\n",
    "    new_words = []\n",
    "    for word in words_list:\n",
    "        if word not in stopwords_list:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def lemmatizer(words_list):\n",
    "    \"\"\"\"\"\"\n",
    "    obj_lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    words_lemma = []\n",
    "    for word in words_list:\n",
    "        words_lemma.append(obj_lemmatizer.lemmatize(word,pos=nltk.corpus.wordnet.VERB))\n",
    "    return words_lemma\n",
    "\n",
    "def stem_text(words_list):\n",
    "    \"\"\"\"\"\"\n",
    "    p_stem = nltk.stem.PorterStemmer()\n",
    "    words_stem = []\n",
    "    for word in words_list:\n",
    "        words_stem.append(p_stem.stem(word))\n",
    "    return words_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_prep(text, clean_text=True, stopwords_remove=True, exec_lemmatizer=True, exec_stem=False, text_lower=False, stopwords_list=[], language='english',\n",
    "              preserve_line=False, regex_chars_clean=\"[^a-zA-Z]+\", replace_chars_clean='', min_word_len=1):\n",
    "    \n",
    "    \"\"\"Text preparation.\"\"\"\n",
    "    \n",
    "    text_preparation = text_tokenize(text, language=language, preserve_line=preserve_line)\n",
    "    if clean_text:\n",
    "        text_preparation = clean_text_regex(words_list=text_preparation,\n",
    "                                            regex=regex_chars_clean,\n",
    "                                            replace=replace_chars_clean,\n",
    "                                            min_word_len=min_word_len)\n",
    "    if stopwords_remove:\n",
    "        text_preparation = remove_stopwords(words_list=text_preparation,\n",
    "                                            stopwords_list=stopwords_list)\n",
    "    if exec_lemmatizer:\n",
    "        text_preparation = lemmatizer(words_list=text_preparation)\n",
    "    if exec_stem:\n",
    "        text_preparation = stem_text(words_list=text_preparation)\n",
    "    text_preparation = ' '.join(text_preparation)\n",
    "    if text_lower:\n",
    "        text_preparation = text_preparation.lower()\n",
    "    return text_preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_prep_column(colum_df):\n",
    "    \"\"\"\"\"\"\n",
    "    f_prep_text = lambda text_data: text_prep(text=text_data, clean_text=True, stopwords_remove=True, exec_lemmatizer=True, exec_stem=False, \n",
    "                                            stopwords_list=nltk.corpus.stopwords.words('english'), language='english', preserve_line=False,\n",
    "                                            regex_chars_clean=\"[^a-zA-Z]+\", replace_chars_clean='', min_word_len=1, text_lower=True)\n",
    "    colum_df = colum_df.apply(lambda e: e if pd.isna(e) else f_prep_text(e))\n",
    "    return colum_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_dfs['df_doc_info']['acknowledgement_prep'] = text_prep_column(dict_dfs['df_doc_info']['acknowledgement'])\n",
    "dict_dfs['df_doc_info']['abstract_prep'] = text_prep_column(dict_dfs['df_doc_info']['abstract'])\n",
    "dict_dfs['df_doc_info']['body_prep'] = text_prep_column(dict_dfs['df_doc_info']['body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando BOW e TFIDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = dict_dfs['df_doc_info']['abstract_prep'].fillna(' ').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_bow = CountVectorizer(encoding=\"utf-8\",\n",
    "                          stop_words=\"english\",\n",
    "                          strip_accents=\"ascii\",\n",
    "                          lowercase=True, \n",
    "                          preprocessor=None,\n",
    "                          tokenizer=None,\n",
    "                          token_pattern=r\"\"\"(?u)\\b\\w\\w+\\b\"\"\",\n",
    "                          ngram_range=(1,2), # Unigram and bigram\n",
    "                          analyzer=\"word\",\n",
    "                          max_df=1.0,\n",
    "                          min_df=2, # May have at least 2 frequency\n",
    "                          max_features=None, \n",
    "                          vocabulary= None, \n",
    "                          binary=False, \n",
    "                          dtype=np.int64)\n",
    "\n",
    "obj_tfidf = TfidfVectorizer(encoding=\"utf-8\",\n",
    "                            stop_words=\"english\",\n",
    "                            strip_accents=\"ascii\",\n",
    "                            lowercase=True, \n",
    "                            preprocessor=None,\n",
    "                            tokenizer=None,\n",
    "                            token_pattern=r\"\"\"(?u)\\b\\w\\w+\\b\"\"\",\n",
    "                            ngram_range=(1,2), # Unigram and bigram\n",
    "                            analyzer=\"word\",\n",
    "                            max_df=1.0,\n",
    "                            min_df=2, # May have at least 2 frequency\n",
    "                            max_features=None, \n",
    "                            vocabulary= None, \n",
    "                            binary=False, \n",
    "                            dtype=np.float64, \n",
    "                            norm='l2', \n",
    "                            use_idf=True, \n",
    "                            smooth_idf=True, \n",
    "                            sublinear_tf=False)\n",
    "\n",
    "obj_bow = obj_bow.fit(raw_documents=documents)\n",
    "bow_matrix = obj_bow.transform(documents)\n",
    "\n",
    "obj_tfidf = obj_tfidf.fit(raw_documents=documents)\n",
    "tfidf_matrix = obj_tfidf.transform(documents)\n",
    "\n",
    "bow_matrix = bow_matrix.todense()\n",
    "tfidf_matrix = tfidf_matrix.todense()\n",
    "\n",
    "df_bow = pd.DataFrame(bow_matrix, columns=obj_bow.get_feature_names())\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix, columns=obj_tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trabalhando com a similaridade de Cosseno:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_bow_sim = cosine_similarity(df_bow, df_bow)\n",
    "cos_tfidf_sim = cosine_similarity(df_tfidf, df_tfidf)\n",
    "\n",
    "df_cos_bow_sim = pd.DataFrame(cos_bow_sim,\n",
    "                              columns=dict_dfs['df_doc_info'].index.tolist(),\n",
    "                              index=dict_dfs['df_doc_info'].index.tolist())\n",
    "\n",
    "df_cos_tfidf_sim = pd.DataFrame(cos_tfidf_sim,\n",
    "                                columns=dict_dfs['df_doc_info'].index.tolist(),\n",
    "                                index=dict_dfs['df_doc_info'].index.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gerando grafo de similaridade de cosseno:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sim_matrix(matrix, percentil=\"75%\", value_min=0, value_max=1):\n",
    "    \"\"\"\"\"\"\n",
    "    list_elements = []\n",
    "    for colum in df_cos_tfidf_sim.columns.tolist():\n",
    "        list_elements += df_cos_tfidf_sim[colum].tolist()\n",
    "    sim_describe = pd.Series(list_elements).describe(percentiles=np.arange(0, 1, 0.001))\n",
    "    del list_elements\n",
    "    \n",
    "    filter_matrix = sim_describe[percentil]\n",
    "    \n",
    "    list_filter = []\n",
    "    for i,row in matrix.iterrows():\n",
    "        for j in row.index:\n",
    "            value = matrix.loc[i,j]\n",
    "            logic_filter = value>=filter_matrix and value>value_min and value<value_max\n",
    "            if not pd.isna(value) and logic_filter:\n",
    "                dictCell = {\"doc_a\":i,\"doc_b\":j,'value':matrix.loc[i,j]}\n",
    "                list_filter.append(dictCell)\n",
    "    df_maxtrix_filter = pd.DataFrame(list_filter)\n",
    "    del list_filter\n",
    "    \n",
    "    return df_maxtrix_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sim_graph(matrix,\n",
    "                   node_data,\n",
    "                   source_column=\"doc_a\",\n",
    "                   to_column=\"doc_b\",\n",
    "                   value_column=\"value\",\n",
    "                   height=\"500px\",\n",
    "                   width=\"500px\",\n",
    "                   directed=False,\n",
    "                   notebook=False,\n",
    "                   bgcolor=\"#ffffff\",\n",
    "                   font_color=False,\n",
    "                   layout=None,\n",
    "                   heading=\"\",\n",
    "                   path_graph=\"./\", \n",
    "                   name_file=\"graph.html\"):\n",
    "    \"\"\"\"\"\"\n",
    "    graph = Network(height=height,\n",
    "                    width=width,\n",
    "                    directed=directed,\n",
    "                    notebook=notebook,\n",
    "                    bgcolor=bgcolor,\n",
    "                    font_color=font_color,\n",
    "                    layout=layout,\n",
    "                    heading=heading)\n",
    "\n",
    "    for i, row in node_data.iterrows():\n",
    "        \n",
    "        article_id = str(row['pdf_md5'])\n",
    "        article_title = str(row['title_head'])\n",
    "        article_abstract_short = str(row['abstract_short'])\n",
    "        article_date = str(row['date_head'])\n",
    "        article_number_authors = str(row['author_count'])\n",
    "        article_number_citations = str(row['citation_count'])\n",
    "        article_doi = str(row['doi_head'])\n",
    "        article_file_name = str(row['file_name'])\n",
    "        article_file_path = str(row['file'])\n",
    "        \n",
    "        title_html = f\"\"\"Article Title:{article_title}\n",
    "                         Article Date:{article_date}\n",
    "                         Article Number Authors:{article_number_authors}\n",
    "                         Article Number Citations:{article_number_citations}\n",
    "                         Article DOI:{article_doi}\n",
    "                         Article File Name:{article_file_name}\"\"\"\n",
    "        \n",
    "        graph.add_node(n_id=article_id, \n",
    "                       label=f\"Node ID: {str(article_id)[0:4]}\", \n",
    "                       borderWidth=1, \n",
    "                       borderWidthSelected=2, \n",
    "                       #brokenImage=\"url\", \n",
    "                       #group=\"a\", \n",
    "                       #hidden=False, \n",
    "                       #image=\"url\", \n",
    "                       #labelHighlightBold=True, \n",
    "                       #level=1, \n",
    "                       #mass=1, \n",
    "                       #physics=True,\n",
    "                       shape=\"dot\", # image, circularImage, diamond, dot, star, triangle, triangleDown, square and icon\n",
    "                       size=1, \n",
    "                       title=title_html,  \n",
    "                       #x=0.5, \n",
    "                       #y=1.0)\n",
    "                       value=1)\n",
    "        \n",
    "    for i,row in matrix.iterrows():\n",
    "        \n",
    "        graph.add_edge(source=row[source_column],\n",
    "                       to=row[to_column],\n",
    "                       value=round(row[value_column],1),\n",
    "                       title=row[value_column])\n",
    "                       #width=row['value'],\n",
    "                       #arrowStrikethrough=False,\n",
    "                       #physics=False,\n",
    "                       #hidden=False)\n",
    "    \n",
    "    graph.force_atlas_2based(gravity=-50,\n",
    "                             central_gravity=0.01,\n",
    "                             spring_length=360,\n",
    "                             spring_strength=0.08,\n",
    "                             damping=0.4,\n",
    "                             overlap=0)\n",
    "    \n",
    "    graph.save_graph(os.path.join(path_graph, name_file))\n",
    "    graph.show_buttons(filter_=['physics'])\n",
    "    graph.show(name_file)\n",
    "    \n",
    "    return graph   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cos_bow_sim_filter = filter_sim_matrix(df_cos_bow_sim, percentil=\"99%\", value_min=0, value_max=0.99)\n",
    "df_cos_bow_sim_filter = df_cos_bow_sim_filter.nlargest(300,'value')\n",
    "\n",
    "df_cos_tfidf_sim_filter = filter_sim_matrix(df_cos_tfidf_sim, percentil=\"99%\", value_min=0, value_max=0.99)\n",
    "df_cos_tfidf_sim_filter = df_cos_tfidf_sim_filter.nlargest(300,'value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_data(dict_dfs):\n",
    "    \n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    # Selecting head article data\n",
    "    cols_head = ['title_head', 'doi_head', 'date_head',]\n",
    "    head_data = dict_dfs['df_doc_head'].loc[:,cols_head].reset_index().copy()\n",
    "    head_data['title_head'] = head_data['title_head'].apply(lambda e: str(e)[0:50] + \"...\" if len(str(e)) > 50 else str(e))\n",
    "\n",
    "    # Selecting head article data\n",
    "    cols_info = ['abstract','file']\n",
    "    doc_info_data = dict_dfs['df_doc_info'].loc[:,cols_info].reset_index().copy()\n",
    "    doc_info_data['file_name'] = doc_info_data['file'].apply(lambda e: os.path.split(e)[-1])\n",
    "    doc_info_data['abstract_short'] = doc_info_data['abstract'].apply(lambda e: str(e)[0:20] + \"...\" if len(str(e)) > 20 else str(e))\n",
    "    doc_info_data.drop(labels=['abstract'], axis=1, inplace=True)\n",
    "\n",
    "    # Selecting authors information\n",
    "    authors_data = dict_dfs['df_doc_authors'].reset_index()\n",
    "    authors_data = authors_data.groupby(by=['pdf_md5'], as_index=False)['full_name_author'].count()\n",
    "    authors_data.rename(columns={'full_name_author':'author_count'}, inplace=True)\n",
    "\n",
    "    # Selecting citations information\n",
    "    citations_data = dict_dfs['df_doc_citations'].reset_index()\n",
    "    citations_data = citations_data.groupby(by=['pdf_md5'], as_index=False)['index_citation'].count()\n",
    "    citations_data.rename(columns={'index_citation':'citation_count'}, inplace=True)\n",
    "\n",
    "    nodes = dict_dfs['df_doc_info'].reset_index()['pdf_md5'].tolist()\n",
    "    df_nodes = pd.DataFrame(nodes, columns=['pdf_md5'])\n",
    "\n",
    "    df_nodes = df_nodes.merge(head_data, how='left', on='pdf_md5')\n",
    "    df_nodes = df_nodes.merge(doc_info_data, how='left', on='pdf_md5')\n",
    "    df_nodes = df_nodes.merge(authors_data, how='left', on='pdf_md5')\n",
    "    df_nodes = df_nodes.merge(citations_data, how='left', on='pdf_md5')\n",
    "    \n",
    "    return df_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'pdf_md5'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-b66f7f5ec9e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Selecting authors information\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mauthors_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict_dfs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'df_doc_authors'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mauthors_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mauthors_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pdf_md5'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'full_name_author'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mauthors_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'full_name_author'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'author_count'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\vierb\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mgroupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001b[0m\n\u001b[0;32m   7710\u001b[0m         \u001b[1;31m# error: Argument \"squeeze\" to \"DataFrameGroupBy\" has incompatible type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7711\u001b[0m         \u001b[1;31m# \"Union[bool, NoDefault]\"; expected \"bool\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7712\u001b[1;33m         return DataFrameGroupBy(\n\u001b[0m\u001b[0;32m   7713\u001b[0m             \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7714\u001b[0m             \u001b[0mkeys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\vierb\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated, dropna)\u001b[0m\n\u001b[0;32m    880\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrouper\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_grouper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 882\u001b[1;33m             grouper, exclusions, obj = get_grouper(\n\u001b[0m\u001b[0;32m    883\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m                 \u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\vierb\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\grouper.py\u001b[0m in \u001b[0;36mget_grouper\u001b[1;34m(obj, key, axis, level, sort, observed, mutated, validate, dropna)\u001b[0m\n\u001b[0;32m    880\u001b[0m                 \u001b[0min_axis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgpr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    881\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 882\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    883\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mgpr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m             \u001b[1;31m# Add key to exclusions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'pdf_md5'"
     ]
    }
   ],
   "source": [
    "# Selecting head article data\n",
    "cols_head = ['title_head', 'doi_head', 'date_head',]\n",
    "head_data = dict_dfs['df_doc_head'].loc[:,cols_head].reset_index().copy()\n",
    "head_data['title_head'] = head_data['title_head'].apply(lambda e: str(e)[0:50] + \"...\" if len(str(e)) > 50 else str(e))\n",
    "\n",
    "# Selecting head article data\n",
    "cols_info = ['abstract','file']\n",
    "doc_info_data = dict_dfs['df_doc_info'].loc[:,cols_info].reset_index().copy()\n",
    "doc_info_data['file_name'] = doc_info_data['file'].apply(lambda e: os.path.split(e)[-1])\n",
    "doc_info_data['abstract_short'] = doc_info_data['abstract'].apply(lambda e: str(e)[0:20] + \"...\" if len(str(e)) > 20 else str(e))\n",
    "doc_info_data.drop(labels=['abstract'], axis=1, inplace=True)\n",
    "\n",
    "# Selecting authors information\n",
    "authors_data = dict_dfs['df_doc_authors'].reset_index()\n",
    "authors_data = authors_data.groupby(by=['pdf_md5'], as_index=False)['full_name_author'].count()\n",
    "authors_data.rename(columns={'full_name_author':'author_count'}, inplace=True)\n",
    "\n",
    "# Selecting citations information\n",
    "citations_data = dict_dfs['df_doc_citations'].reset_index()\n",
    "citations_data = citations_data.groupby(by=['pdf_md5'], as_index=False)['index_citation'].count()\n",
    "citations_data.rename(columns={'index_citation':'citation_count'}, inplace=True)\n",
    "\n",
    "nodes = list(set(df_cos_bow_sim_filter.doc_a.tolist()+df_cos_bow_sim_filter.doc_b.tolist()))\n",
    "df_nodes = pd.DataFrame(nodes, columns=['pdf_md5'])\n",
    "\n",
    "df_nodes = df_nodes.merge(head_data, how='left', on='pdf_md5')\n",
    "df_nodes = df_nodes.merge(doc_info_data, how='left', on='pdf_md5')\n",
    "df_nodes = df_nodes.merge(authors_data, how='left', on='pdf_md5')\n",
    "df_nodes = df_nodes.merge(citations_data, how='left', on='pdf_md5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_graph = make_sim_graph(matrix=df_cos_bow_sim_filter,\n",
    "                           node_data=df_nodes,\n",
    "                           source_column=\"doc_a\",\n",
    "                           to_column=\"doc_b\",\n",
    "                           value_column=\"value\",\n",
    "                           height=\"1000px\",\n",
    "                           width=\"1000px\",\n",
    "                           directed=True,\n",
    "                           notebook=False,\n",
    "                           bgcolor=\"#ffffff\",\n",
    "                           font_color=False,\n",
    "                           layout=None,\n",
    "                           heading=\"\",\n",
    "                           path_graph=\"./\", \n",
    "                           name_file=\"graph.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_model = KeyBERT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aleatory_color():\n",
    "    \n",
    "\t'''Returns color in hex format'''\n",
    " \n",
    "\tred_int = random.randint(0,255)\n",
    "\tgreen_int = random.randint(0,255)\n",
    "\tblue_int = random.randint(0,255)\n",
    " \n",
    "\treturn '#{:02X}{:02X}{:02X}'.format(red_int, green_int, blue_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_keywords_graph(edges_key_articles,\n",
    "                        node_data,\n",
    "                        node_keywords_data,\n",
    "                        source_column=\"keyword\",\n",
    "                        to_column=\"pdf_md5\",\n",
    "                        value_column=\"value\",\n",
    "                        height=\"500px\",\n",
    "                        width=\"500px\",\n",
    "                        directed=False,\n",
    "                        notebook=False,\n",
    "                        bgcolor=\"#ffffff\",\n",
    "                        font_color=False,\n",
    "                        layout=None,\n",
    "                        heading=\"\",\n",
    "                        path_graph=\"./\", \n",
    "                        name_file=\"graph_keyword.html\"):\n",
    "    \n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    graph = Network(height=height,\n",
    "                    width=width,\n",
    "                    directed=directed,\n",
    "                    notebook=notebook,\n",
    "                    bgcolor=bgcolor,\n",
    "                    font_color=font_color,\n",
    "                    layout=layout,\n",
    "                    heading=heading)\n",
    "\n",
    "    for i, row in node_data.iterrows():\n",
    "        \n",
    "        article_id = str(row['pdf_md5'])\n",
    "        article_title = str(row['title_head'])\n",
    "        article_abstract_short = str(row['abstract_short'])\n",
    "        article_date = str(row['date_head'])\n",
    "        article_number_authors = str(row['author_count'])\n",
    "        article_number_citations = str(row['citation_count'])\n",
    "        article_doi = str(row['doi_head'])\n",
    "        article_file_name = str(row['file_name'])\n",
    "        article_file_path = str(row['file'])\n",
    "        \n",
    "        title_html = f\"\"\"Article Title:{article_title}\n",
    "                         Article Date:{article_date}\n",
    "                         Article Number Authors:{article_number_authors}\n",
    "                         Article Number Citations:{article_number_citations}\n",
    "                         Article DOI:{article_doi}\n",
    "                         Article File Name:{article_file_name}\"\"\"\n",
    "        \n",
    "        graph.add_node(n_id=article_id, \n",
    "                       label=f\"Node ID: {str(article_id)[0:4]}\", \n",
    "                       borderWidth=1, \n",
    "                       borderWidthSelected=2, \n",
    "                       #brokenImage=\"url\", \n",
    "                       #group=\"a\", \n",
    "                       #hidden=False, \n",
    "                       #image=\"url\", \n",
    "                       #labelHighlightBold=True, \n",
    "                       #level=1, \n",
    "                       #mass=1, \n",
    "                       #physics=True,\n",
    "                       shape=\"dot\", # image, circularImage, diamond, dot, star, triangle, triangleDown, square and icon\n",
    "                       size=1, \n",
    "                       title=title_html,  \n",
    "                       #x=0.5, \n",
    "                       #y=1.0)\n",
    "                       value=1)\n",
    "        \n",
    "    for i, row in node_keywords_data.iterrows():\n",
    "        \n",
    "        keyword_id = str(row['keyword'])\n",
    "        article_count = row['article_count']\n",
    "        value_sum = row['value_sum']\n",
    "        value_mean = row['value_mean']\n",
    "        \n",
    "        title_html = f\"\"\"KeyWord: {keyword_id}\n",
    "                         Article Count: {article_count}\n",
    "                         Value Sum: {value_sum}\n",
    "                         Value Mean: {value_mean}\n",
    "                      \"\"\"\n",
    "        \n",
    "        graph.add_node(n_id=keyword_id, \n",
    "                       label=keyword_id, \n",
    "                       borderWidth=2, \n",
    "                       borderWidthSelected=4,\n",
    "                       color=get_aleatory_color(),\n",
    "                       #brokenImage=\"url\", \n",
    "                       #group=\"a\", \n",
    "                       #hidden=False, \n",
    "                       #image=\"url\", \n",
    "                       #labelHighlightBold=True, \n",
    "                       #level=1, \n",
    "                       #mass=1, \n",
    "                       #physics=True,\n",
    "                       shape=\"box\", # image, circularImage, diamond, dot, star, triangle, triangleDown, square and icon, box, text\n",
    "                       size=article_count, \n",
    "                       title=title_html,  \n",
    "                       #x=0.5, \n",
    "                       #y=1.0)\n",
    "                       value=article_count)\n",
    "    \n",
    "    for i, row in edges_key_articles.iterrows():\n",
    "        \n",
    "        graph.add_edge(source=row[source_column],\n",
    "                       to=row[to_column],\n",
    "                       value=round(row[value_column],1),\n",
    "                       title=row[value_column])\n",
    "                       #width=row['value'],\n",
    "                       #arrowStrikethrough=False,\n",
    "                       #physics=False,\n",
    "                       #hidden=False)\n",
    "    \n",
    "    graph.force_atlas_2based(gravity=-50,\n",
    "                             central_gravity=0.01,\n",
    "                             spring_length=360,\n",
    "                             spring_strength=0.08,\n",
    "                             damping=0.4,\n",
    "                             overlap=0)\n",
    "    \n",
    "    graph.save_graph(os.path.join(path_graph, name_file))\n",
    "    graph.show_buttons(filter_=['physics'])\n",
    "    graph.show(name_file)\n",
    "    \n",
    "    return graph   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_keywords = {}\n",
    "id_column = 'pdf_md5'\n",
    "text_column = 'abstract'\n",
    "col_select = [id_column,text_column]\n",
    "docs = dict_dfs['df_doc_info'].reset_index().loc[:, col_select]\n",
    "\n",
    "list_keywordsdf = []\n",
    "list_keywordsdf_article = []\n",
    "for i, row in docs.iterrows():\n",
    "    \n",
    "    doc = str(row[text_column])\n",
    "    id = row[id_column]\n",
    "    \n",
    "    keywords_unigram = kw_model.extract_keywords(doc, keyphrase_ngram_range=(1, 1), stop_words='english', highlight=False, top_n=10)\n",
    "    if len(keywords_unigram):\n",
    "        df_unigram = pd.DataFrame([{'keyword':v[0],'value':v[1]} for v in keywords_unigram])\n",
    "    else:\n",
    "        df_unigram = pd.DataFrame([], columns=['keyword','value'])\n",
    "\n",
    "    keywords_bigram = kw_model.extract_keywords(doc, keyphrase_ngram_range=(2, 2), stop_words='english', highlight=False, top_n=10)\n",
    "    if len(keywords_bigram):\n",
    "        df_bigram = pd.DataFrame([{'keyword':v[0],'value':v[1]} for v in keywords_bigram])\n",
    "    else:\n",
    "        df_bigram = pd.DataFrame([], columns=['keyword','value'])\n",
    "\n",
    "    keywords_trigam = kw_model.extract_keywords(doc, keyphrase_ngram_range=(3, 3), stop_words='english', highlight=False, top_n=10)\n",
    "    if len(keywords_bigram):\n",
    "        df_trigram = pd.DataFrame([{'keyword':v[0],'value':v[1]} for v in keywords_trigam])\n",
    "    else:\n",
    "        df_trigram = pd.DataFrame([], columns=['keyword','value'])\n",
    "    \n",
    "    dict_keywords[id] = {'unigram':df_unigram, 'bigram':df_bigram, 'trigram':df_trigram}\n",
    "    \n",
    "    df_article_keywords = pd.concat([df_unigram, df_bigram, df_trigram])\n",
    "    df_article_keywords[id_column] = id\n",
    "    df_article_keywords = df_article_keywords.loc[:,[id_column,'keyword', 'value']].copy()\n",
    "    list_keywordsdf_article.append(df_article_keywords)\n",
    "    \n",
    "    df_unigram.rename(columns={'keyword':'keyword_unigram','value':'value_unigram'}, inplace=True)\n",
    "    df_bigram.rename(columns={'keyword':'keyword_bigram','value':'value_bigram'}, inplace=True)\n",
    "    df_trigram.rename(columns={'keyword':'keyword_trigram','value':'value_trigram'}, inplace=True)\n",
    "    \n",
    "    df_keywords_article = pd.concat([df_unigram, df_bigram, df_trigram], axis=1)\n",
    "    dict_keywords[id]['df_keywords'] = df_keywords_article\n",
    "    \n",
    "    list_keywordsdf.append(df_keywords_article)\n",
    "    \n",
    "df_keywords_all = pd.concat(list_keywordsdf)\n",
    "df_keywords_all.dropna(inplace=True)\n",
    "\n",
    "df_article_keywords_all = pd.concat(list_keywordsdf_article)\n",
    "df_article_keywords_all.dropna(inplace=True)\n",
    "\n",
    "df_keywords_unigram = df_keywords_all.groupby(by=['keyword_unigram'], as_index=False)['value_unigram'].sum()\n",
    "df_keywords_unigram.sort_values(by='value_unigram', ascending=False, inplace=True)\n",
    "\n",
    "df_keywords_bigram = df_keywords_all.groupby(by=['keyword_bigram'], as_index=False)['value_bigram'].sum()\n",
    "df_keywords_bigram.sort_values(by='value_bigram', ascending=False, inplace=True)\n",
    "\n",
    "df_keywords_trigram = df_keywords_all.groupby(by=['keyword_trigram'], as_index=False)['value_trigram'].sum()\n",
    "df_keywords_trigram.sort_values(by='value_trigram', ascending=False, inplace=True)\n",
    "\n",
    "df_keywords_all = pd.concat([df_keywords_unigram, df_keywords_bigram, df_keywords_trigram], axis=1)\n",
    "df_keywords_all = df_keywords_all.head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_keys_node_data(grupo):\n",
    "    \"\"\"\"\"\"\n",
    "    dictAgg = {}\n",
    "    dictAgg['keyword'] = grupo['keyword'].iat[0]\n",
    "    dictAgg['article_count'] = grupo['pdf_md5'].shape[0]\n",
    "    dictAgg['value_sum'] = grupo['value'].sum()\n",
    "    dictAgg['value_mean'] = grupo['value'].mean()\n",
    "    \n",
    "    return pd.Series(dictAgg)\n",
    "\n",
    "df_keyword_data = df_article_keywords_all.groupby(by=['keyword'], as_index=False).apply(agg_keys_node_data)\n",
    "top_keywords = 5 # int(df_keyword_data.shape[0]*0.1)\n",
    "df_keyword_data = df_keyword_data.sort_values(by=['article_count'], ascending=False).head(top_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting edges that contains top keywords\n",
    "filtro = (df_article_keywords_all.keyword.isin(df_keyword_data.keyword.tolist()))\n",
    "df_art_key_all = df_article_keywords_all.loc[(filtro)].copy()\n",
    "\n",
    "# Selecting nodes in the list of selected edges\n",
    "df_nodes = get_node_data(dict_dfs)\n",
    "\n",
    "filtro = (df_nodes['pdf_md5'].isin(df_art_key_all['pdf_md5'].tolist()))\n",
    "df_nodes = df_nodes.loc[(filtro)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_graph = make_keywords_graph(edges_key_articles=df_art_key_all,\n",
    "                                     node_data=df_nodes,\n",
    "                                     node_keywords_data=df_keyword_data,\n",
    "                                     source_column=\"keyword\",\n",
    "                                     to_column=\"pdf_md5\",\n",
    "                                     value_column=\"value\",\n",
    "                                     height=\"1000px\",\n",
    "                                     width=\"1000px\",\n",
    "                                     directed=False,\n",
    "                                     notebook=False,\n",
    "                                     bgcolor=\"#ffffff\",\n",
    "                                     font_color=False,\n",
    "                                     layout=None,\n",
    "                                     heading=\"\",\n",
    "                                     path_graph=\"./\", \n",
    "                                     name_file=\"graph_keyword.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering, DBSCAN, KMeans, OPTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'3.0'.isnumeric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'3.0'.isalnum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a', 'b', 'c')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(['a','b','c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 if not None else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20220613195307'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c1a9770e27579e7348d9a860a8519a7eb2fc807ceb8202e6d163f2591bed9881"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
