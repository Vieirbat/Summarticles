# --------------------------------------------------------------------------------------------------------
# O streamlit chat √© um plugin do Streamlit para criar chats
# J√° vem com uma cole√ß√£o de utilit√°rios para chat no streamlit
# https://github.com/AI-Yash/st-chat

from streamlit_chat import message

# ------------------------------------------------------------------------------------------------------
# Essa cadeia coleta o hist√≥rico do bate-papo (uma lista de mensagens) e novas perguntas e, em seguida, 
# retorna uma resposta a essas perguntas. O algoritmo para esta cadeia consiste em tr√™s partes:

# 1. Use o hist√≥rico de bate-papo e a nova pergunta para criar uma ‚Äúpergunta independente‚Äù. 
#    Isso √© feito para que esta quest√£o possa ser passada para a etapa de recupera√ß√£o para buscar documentos relevantes. 
#    Se apenas a nova pergunta foi transmitida, pode faltar contexto relevante. 
#    Se toda a conversa for recuperada, pode haver informa√ß√µes desnecess√°rias que poderiam desviar a aten√ß√£o da recupera√ß√£o.

# 2. Esta nova pergunta √© passada ao recuperador e os documentos relevantes s√£o devolvidos.

# 3. Os documentos recuperados s√£o passados ‚Äã‚Äãpara um LLM juntamente com a nova pergunta (comportamento padr√£o)
# ou a pergunta original e o hist√≥rico de bate-papo para gerar uma resposta final.
# https://python.langchain.com/docs/modules/chains/

from langchain.chains import ConversationalRetrievalChain

# --------------------------------------------------------------------------------------------------------
# Aqui estamos olhando os modelos de embbedding do langchain, especificamente os embeddings do huggingface
# O embedding serve para transformar nossos textos em n√∫meros, esses embeddings j√° est√£o pr√©-treinados
# https://python.langchain.com/docs/integrations/text_embedding/huggingfacehub/
# https://huggingface.co/blog/getting-started-with-embeddings model names
# https://huggingface.co/sentence-transformers and https://www.sbert.net/
# Aqui est√° a lista de modelos pr√©-treinados: https://www.sbert.net/docs/pretrained_models.html

# As incorpora√ß√µes criam uma representa√ß√£o vetorial de um trecho de texto. 
# Isso √© √∫til porque significa que podemos pensar no texto no espa√ßo vetorial e fazer coisas como pesquisa sem√¢ntica,
# onde procuramos trechos de texto mais semelhantes no espa√ßo vetorial.
 
from langchain.embeddings import HuggingFaceEmbeddings

# --------------------------------------------------------------------------------------------------------
# Large Language Models (LLMs) s√£o um componente central do LangChain. L
# angChain n√£o oferece seus pr√≥prios LLMs, mas fornece uma interface padr√£o para interagir com muitos LLMs diferentes. 
# Para ser mais espec√≠fico, essa interface recebe como entrada uma string e retorna uma string.

# O principal objetivo llama.cpp √© permitir a infer√™ncia LLM com configura√ß√£o m√≠nima e desempenho de √∫ltima gera√ß√£o
# em uma ampla variedade de hardware - localmente e na nuvem.

# llama-cpp-python √© uma liga√ß√£o Python para llama.cpp .
# Ele suporta infer√™ncia para muitos modelos de LLMs, que podem ser acessados ‚Äã‚Äãem Hugging Face .

from langchain_community.llms import LlamaCpp # pip install langchain-community --upgrade

# --------------------------------------------------------------------------------------------------------
# Este divisor de texto √© o recomendado para texto gen√©rico. √â parametrizado por uma lista de caracteres. 
# Ele tenta dividi-los em ordem at√© que os peda√ßos sejam pequenos o suficiente. A lista padr√£o √© ["\n\n", "\n", " ", ""]. 
# Isso tem o efeito de tentar manter todos os par√°grafos (e depois as senten√ßas e depois as palavras) juntos pelo maior tempo poss√≠vel, 
# j√° que esses geralmente pareceriam ser os trechos de texto semanticamente mais fortes.
# https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter/
# Entenda melhor na documenta√ß√£o: https://python.langchain.com/docs/modules/data_connection/document_transformers/

from langchain.text_splitter import RecursiveCharacterTextSplitter

# --------------------------------------------------------------------------------------------------------
# https://python.langchain.com/docs/modules/data_connection/vectorstores/

# Uma das maneiras mais comuns de armazenar e pesquisar dados n√£o estruturados √© incorpor√°-los 
# e armazenar os vetores de incorpora√ß√£o resultantes e, em seguida, no momento da consulta, 
# incorporar a consulta n√£o estruturada e recuperar os vetores de incorpora√ß√£o que s√£o 'mais semelhantes' √† consulta incorporada.
# Um armazenamento de vetores se encarrega de armazenar dados incorporados e realizar pesquisas de vetores para voc√™.

# Facebook AI Similarity Search (Faiss) √© uma biblioteca para pesquisa eficiente de similaridade e agrupamento de vetores densos. 
# Cont√©m algoritmos que pesquisam em conjuntos de vetores de qualquer tamanho, at√© aqueles que possivelmente n√£o cabem na RAM. 
# Ele tamb√©m cont√©m c√≥digo de suporte para avalia√ß√£o e ajuste de par√¢metros.
# https://faiss.ai/
# chromadb √© outra forma de fazer isso

# As incorpora√ß√µes criam uma representa√ß√£o vetorial de um trecho de texto. 
# Isso √© √∫til porque significa que podemos pensar no texto no espa√ßo vetorial e fazer coisas como pesquisa sem√¢ntica,
# onde procuramos trechos de texto mais semelhantes no espa√ßo vetorial.

from langchain_community.vectorstores import FAISS

# --------------------------------------------------------------------------------------------------------
# A maioria dos aplicativos LLM possui uma interface conversacional. 
# Um componente essencial de uma conversa √© ser capaz de fazer refer√™ncia a informa√ß√µes introduzidas anteriormente na conversa.
# No m√≠nimo, um sistema de conversa√ß√£o deve ser capaz de acessar diretamente alguma janela de mensagens anteriores.

# Chamamos essa capacidade de armazenar informa√ß√µes sobre intera√ß√µes passadas de ‚Äúmem√≥ria‚Äù.
# LangChain fornece muitos utilit√°rios para adicionar mem√≥ria a um sistema. 
# Esses utilit√°rios podem ser usados ‚Äã‚Äãsozinhos ou incorporados perfeitamente em uma cadeia.

# https://python.langchain.com/docs/modules/memory/ 

from langchain.memory import ConversationBufferMemory

from langchain_core.documents.base import Document

# --------------------------------------------------------------------------------------------------------
# Deepseek 

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_ollama import OllamaEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama.llms import OllamaLLM

import openai
from openai import OpenAI

# --------------------------------------------------------------------------------------------------------
# A biblioteca abaixo √© para ler dados de pdfs
# from langchain.document_loaders import PyPDFLoader

# OS √© uma biblioteca para utiliza√ß√£o de recursos do SO
import os

# Para gerar arquivos e diret√≥rios tempor√°rios
import tempfile

def conversation_chat(query, chain, history):
    result = chain({"question": query, "chat_history": history})
    history.append((query, result["answer"]))
    return result["answer"]


def display_chat_history(st, chain):

    reply_container = st.container()
    container = st.container()

    with container:

        with st.form(key='my_form', clear_on_submit=True):
            user_input = st.text_input("Question:", placeholder="Ask about your Article PDF", key='input')
            submit_button = st.form_submit_button(label='Send')

        if submit_button and user_input:
            with st.spinner('Generating response...'):
                output = conversation_chat(user_input, chain, st.session_state['history'])

            st.session_state['past'].append(user_input)
            st.session_state['generated'].append(output)

    if st.session_state['generated']:
        with reply_container:
            for i in range(len(st.session_state['generated'])):
                message(st.session_state["past"][i], is_user=True, key=str(i) + '_user', avatar_style="avataaars-neutral",)
                message(st.session_state["generated"][i], key=str(i), avatar_style="bottts-neutral")


def display_chat_history_openai(st, client):

    with st.chat_message("assistant"):

        messages = [{"role": m["role"], "content": m["content"]} for m in st.session_state['messages']]

        stream = client.chat.completions.create(
            model=st.session_state["openai_model"],
            messages=messages,
            stream=True
        )

        response = st.write_stream(stream)
        
    st.session_state['messages'].append({"role": "assistant", "content": response})


def create_conversational_chain(vector_store, llm, model="local"):

    # Create llm
    # Carregando LLM
    # llm = LlamaCpp(streaming = True,
    #                model_path="llama-2-7b-chat.Q2_K.gguf", # "llama-2-7b-chat.Q5_K_S.gguf", #"mistral-7b-instruct-v0.1.Q4_K_M.gguf",
    #                temperature=0.75,
    #                top_p=1, 
    #                verbose=True,
    #                n_ctx=4096)
    
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

    if model == "local":
        chain = ConversationalRetrievalChain.from_llm(llm=llm, chain_type='stuff',
                                                      retriever=vector_store.as_retriever(search_kwargs={"k": 2}),
                                                      memory=memory)

    return chain


def load_llm_model(model_paph="llama-2-7b-chat.Q2_K.gguf"):
    
    llm = LlamaCpp(streaming=True,
                   model_path=model_paph, # "llama-2-7b-chat.Q5_K_S.gguf", #"mistral-7b-instruct-v0.1.Q4_K_M.gguf",
                   temperature=0.7,
                   tfs=0.95,
                   top_p=1, 
                   verbose=True,
                   n_ctx=10240,
                   top_k=0)
    
    return llm


def make_vector_store(docs):
    
    article_data = tuple(zip(docs['df_doc_info']['abstract'], docs['df_doc_info']['body']))
    article_text = []
    
    for i, texts in article_data:
        # make document for langchain
        doc_str = ' '.join(texts)
        doc = Document(page_content=doc_str)
        article_text.append(doc)
    
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=200)
    text_chunks = text_splitter.split_documents(article_text)

    # Create embeddings
    # model_id = "sentence-transformers/multi-qa-MiniLM-L6-cos-v1"  
    # model_id = "sentence-transformers/all-MiniLM-L6-v2"
    
    model_id = "sentence-transformers/paraphrase-MiniLM-L3-v2"
    embeddings = HuggingFaceEmbeddings(model_name=model_id, 
                                       model_kwargs={'device': 'cpu'})

    # Create vector store
    vector_store = FAISS.from_documents(text_chunks, embedding=embeddings)
    
    return vector_store

# ------------------------------------------------------------------------------------------------------------------------
# Using Llama CCP

    # with st.spinner('üìÑ‚ûûüìÑ  Creating Vector Store...'):
    #     vector_store = make_vector_store(st.session_state['dict_dfs'])
    
    # with st.spinner('üìÑ‚ûûüìÑ  Loading LLM Model...'):
    #     model_file_name = "llama-2-7b-chat.Q2_K.gguf"
    #     path_llm = os.path.join(path,"models",model_file_name)
    #     llm_model = load_llm_model(model_paph=path_llm)
    
    # chain = create_conversational_chain(vector_store, llm_model)
    # display_chat_history(st, chain)

# ------------------------------------------------------------------------------------------------------------------------
# Ollama use below

def get_template():

    template = """
        You are an assistant for question-answering tasks. 
        Use the following articles text data of retrieved context to answer the question. 
        If you don't know the answer, just say that you don't know. 
        Use three sentences maximum and keep the answer concise.
        Question: {question} 
        Context: {context} 
        Answer:
    """

    return template


def get_articles_information(list_documents):
    article_text = []  
    for text in list_documents:
        doc = Document(page_content=text)
        article_text.append(doc)

    return article_text


def split_text(documents):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        add_start_index=True
    )

    return text_splitter.split_documents(documents)


def create_vector_store(documents, model_name="llama3.2:1b"):
    embeddings = OllamaEmbeddings(model=model_name)
    vector_store = InMemoryVectorStore(embeddings)
    chunked_documents = split_text(documents)
    vector_store.add_documents(chunked_documents)

    return vector_store


def rag(st, query, template, vector_store, model):

    related_documents = vector_store.similarity_search(query)

    context = "\n\n".join([doc.page_content for doc in related_documents])
    prompt = ChatPromptTemplate.from_template(template)
    chain = prompt | model

    with st.spinner('üìÑ‚ûûüìÑ  Generating a response...'):
        answer = chain.invoke({"question": query, "context": context})

    return answer


def get_model(model_name='llama3.2:1b'):
    model = OllamaLLM(model=model_name)
    return model


def summachat_ollama(st, model_type='llamma', model_name='llama3.2:1b'):

    if not st.session_state['summachat'].get('template', ''):
        st.session_state['summachat']['template'] = get_template()

    if not st.session_state['summachat'].get('article_text', []):
        article_data = st.session_state['dict_dfs']['df_doc_info']['abstract']
        st.session_state['summachat']['article_text'] = get_articles_information(article_data)

    if not st.session_state['summachat'][f'local_{model_type}'].get('vector_store', False):
        with st.spinner('üìÑ‚ûûüìÑ  Creating Vector Store...'):
            vector_store = create_vector_store(st.session_state['summachat']['article_text'], model_name=model_name)
            st.session_state['summachat'][f'local_{model_type}']['vector_store'] = vector_store

    if not st.session_state['summachat'][f'local_{model_type}'].get('model', False):
        with st.spinner('üìÑ‚ûûüìÑ  Loading LLM Model...'):
            model = get_model(model_name)
            st.session_state['summachat'][f'local_{model_type}']['model'] = model

    question = st.chat_input()
    if question:

        st.chat_message('user').write(question)

        answer = rag(st, question, st.session_state['summachat']['template'],
                     st.session_state['summachat'][f'local_{model_type}']['vector_store'],
                     st.session_state['summachat'][f'local_{model_type}']['model'])

        answer = answer.split('</think>')[-1] if model_type=='deepseek' else answer
        st.chat_message("assistant").write(answer)


def summachat_api(st, model_type="openai"):

    # try:

    if model_type=="openai":
        client = OpenAI(api_key=st.session_state['summachat'][f'api_key_{model_type}'])
    elif model_type=="deepseek":
        client = OpenAI(base_url="https://api.deepseek.com",
                        api_key=st.session_state['summachat'][f'api_key_{model_type}'])

    with st.container():

        if not st.session_state['summachat'].get('article_text', []):
            article_data = st.session_state['dict_dfs']['df_doc_info']['abstract']
            st.session_state['summachat']['article_text_list'] = article_data
        
        if not st.session_state['summachat'].get('context', False):
            context = 'Article Context: ' + '\n\n'.join(st.session_state['summachat']['article_text_list'])
            st.session_state['summachat']['context'] = context

        if not st.session_state['summachat'].get('api_prompt', False):

            api_prompt = """You are an assistant for question-answering tasks. 
                            Use the following articles text data of retrieved context to answer the question. 
                            If you don't know the answer, just say that you don't know. 
                            Use three sentences maximum and keep the answer concise.
                            Context: {context}"""

            st.session_state['summachat']['api_prompt'] = api_prompt

        display_messages(st, st.session_state['summachat']['messages'])

        if prompt := st.chat_input("Ask to your article files, what do you want to know?"):

            # Add user message to chat history
            st.session_state['summachat']['messages'].append({"role": "user", "content": prompt})

            # Display user message in chat message container
            with st.chat_message("user"):
                st.markdown(prompt)

            with st.chat_message("assistant"):

                with st.spinner('üìÑ‚ûûüìÑ Generating a response...'):

                    m0 = [{"role": "system",
                           "content": st.session_state['summachat']['api_prompt'].format(context=st.session_state['summachat']['context'])}]
                    messages = m0 + [{"role": m["role"], "content": m["content"]} for m in st.session_state['summachat']['messages']]

                    stream = client.chat.completions.create(
                        model=st.session_state['summachat'][f"{model_type}_model"],
                        messages=messages,
                        stream=True
                    )
                    
                    response = st.write_stream(stream)
                    
                    st.session_state['summachat']['messages'].append({"role": "assistant", "content": response})

    # except Exception as error:
    #     st.session_state[f'api_key_{model_type}'] = ''
    #     st.session_state['messages'] = []
    #     st.error(error)


def display_messages(st, messages):
    if len(messages):
        for message in messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])


def summachat_variables(st):

    # For SummaChat
    if 'summachat' not in st.session_state:
        st.session_state['summachat'] = {}

    if 'local_deepseek' not in st.session_state['summachat']:
        st.session_state['summachat']['local_deepseek'] = {}

    if 'local_llamma' not in st.session_state['summachat']:
        st.session_state['summachat']['local_llamma'] = {}

    if 'template' not in st.session_state['summachat']:
        st.session_state['summachat']['template'] = ''

    if 'article_text' not in st.session_state['summachat']:
        st.session_state['summachat']['article_text'] = []

    if 'article_text_list' not in st.session_state['summachat']:
        st.session_state['summachat']['article_text_list'] = []

    if 'vector_store' not in st.session_state['summachat']['local_deepseek']:
        st.session_state['summachat']['local_deepseek']['vector_store'] = []

    if 'model' not in st.session_state['summachat']['local_deepseek']:
        st.session_state['summachat']['local_deepseek']['model'] = []

    if 'vector_store' not in st.session_state['summachat']['local_llamma']:
        st.session_state['summachat']['local_llamma']['vector_store'] = []

    if 'model' not in st.session_state['summachat']['local_llamma']:
        st.session_state['summachat']['local_llamma']['model'] = []

    if 'api_prompt' not in st.session_state['summachat']:
        st.session_state['summachat']['api_prompt'] = ''

    if 'context' not in st.session_state['summachat']:
        st.session_state['summachat']['context'] = ''

    if 'history' not in st.session_state['summachat']:
        st.session_state['summachat']['history'] = []

    if 'generated' not in st.session_state['summachat']:
        st.session_state['summachat']['generated'] = ["Welcome to SummaChat, how can I help you? ü§ñ"]

    if 'past' not in st.session_state['summachat']:
        st.session_state['summachat']['past'] = ["Hi!"]

    if 'api_key_openai' not in st.session_state['summachat']:
        st.session_state['summachat']['api_key_openai'] = ''

    if 'api_key_deepseek' not in st.session_state['summachat']:
        st.session_state['summachat']['api_key_deepseek'] = ''

    if 'rb_modelchat' not in st.session_state['summachat']:
        st.session_state['summachat']['rb_modelchat'] = 'Disable SummaChat'

    if "openai_model" not in st.session_state['summachat']:
        st.session_state['summachat']["openai_model"] = "gpt-4o-mini"

    if "deepseek_model" not in st.session_state['summachat']:
        st.session_state['summachat']["deepseek_model"] = "deepseek-chat"

    if "messages" not in st.session_state['summachat']:
        st.session_state['summachat']['messages'] = []